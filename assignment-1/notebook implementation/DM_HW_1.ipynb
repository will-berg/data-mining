{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaYiMURcbe1H"
      },
      "source": [
        "# DM assignment 1 - Finding similar items\n",
        "\n",
        "Konrad Grudzinski - kjgr@kth.se \\\\\n",
        "William Berg - willb@kth.se\n",
        "\n",
        "This notebook implements a technique of finding textually similar documents using shingling, minhashing, Jaccard similarity, and locality-sensitive hashing (LSH). It is implemented in Python, the documents are downloaded using Linux commands directly from the notebook.\n",
        "\n",
        "It can be run directly as is without any parameters in Google Colab, and probably also on a local installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx9qXWJ1-oe1"
      },
      "source": [
        "## Download large text files\n",
        "\n",
        "In this section, various books are downloaded as plain text files from [Project Gutenberg](https://www.gutenberg.org/). A test file called \"shingling_test.txt\" is created from the first few paragraphs of \"Alice in Wonderland\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxhaqRr3Apua"
      },
      "outputs": [],
      "source": [
        "!mkdir text_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8lp549_2mpB",
        "outputId": "9948b7f7-96d3-4255-d3cf-27577fd90b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  438k  100  438k    0     0  22351      0  0:00:20  0:00:20 --:--:--  112k\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/84/pg84.txt > text_files/frankenstein.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_uAea3R-oDz",
        "outputId": "be628a0c-c4e8-488d-8cea-4b84b0214413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  299k  100  299k    0     0  15647      0  0:00:19  0:00:19 --:--:-- 88454\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/64317/pg64317.txt > text_files/thegreatgatsby.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCetctcw-nhs",
        "outputId": "7ec4331b-f5ad-4752-e4d2-68d845285325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  170k  100  170k    0     0   5872      0  0:00:29  0:00:29 --:--:-- 40793\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/11/pg11.txt > text_files/aliceinwonderland.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqlD9UCr-nZy",
        "outputId": "62a26be2-3ddf-43f3-bdd1-ceb0a2b01314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  870k  100  870k    0     0  57117      0  0:00:15  0:00:15 --:--:--  259k\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/345/pg345.txt > text_files/dracula.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7GB-2Dl-nRz",
        "outputId": "f27d1997-141c-4fe5-d7dd-1f02e4d7220d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  454k  100  454k    0     0  22545      0  0:00:20  0:00:20 --:--:--  100k\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/174/pg174.txt > text_files/thepictureofdoriangray.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njnti_fF-nJ7",
        "outputId": "841af54b-f8e0-4920-ed79-c824752e17e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  138k  100  138k    0     0   6279      0  0:00:22  0:00:22 --:--:-- 40898\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/5200/pg5200.txt > text_files/metamorphosis.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kuQn_DY-nCK",
        "outputId": "463d9e4e-a724-4a7a-f6bc-035cd389fb2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  593k  100  593k    0     0  23090      0  0:00:26  0:00:26 --:--:--  190k\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/1661/pg1661.txt > text_files/sherlockholmes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD5P4UtV-m6q",
        "outputId": "cad99e2f-13ec-4af5-d8fa-14a88692562b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  159k  100  159k    0     0  10437      0  0:00:15  0:00:15 --:--:-- 36723\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/43/pg43.txt > text_files/drjekyllandmrhyde.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bFoVX51-mzM",
        "outputId": "c92024db-8379-4316-e9d6-30da666e6bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  300k  100  300k    0     0  18862      0  0:00:16  0:00:16 --:--:-- 73784\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/1232/pg1232.txt > text_files/theprince.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gxi9Ncb-mp3",
        "outputId": "532f0098-411f-4ef6-a8f3-6fcd63cd417c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  754k  100  754k    0     0  23935      0  0:00:32  0:00:32 --:--:--  182k\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/cache/epub/1342/pg1342.txt > text_files/priceandprejudice.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnoQz8rAEL-P",
        "outputId": "ddd23398-6e29-42c5-9e8b-1a4c11aba690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  text_files.zip\n",
            "   creating: text_files/\n",
            "  inflating: text_files/aliceinwonderland.txt  \n",
            "  inflating: text_files/dracula.txt  \n",
            "  inflating: text_files/drjekyllandmrhyde.txt  \n",
            "  inflating: text_files/frankenstein.txt  \n",
            "  inflating: text_files/metamorphosis.txt  \n",
            "  inflating: text_files/priceandprejudice.txt  \n",
            "  inflating: text_files/sherlockholmes.txt  \n",
            "  inflating: text_files/thegreatgatsby.txt  \n",
            "  inflating: text_files/thepictureofdoriangray.txt  \n",
            "  inflating: text_files/theprince.txt  \n"
          ]
        }
      ],
      "source": [
        "# the project gutenberg server often times out, so I downloaded the files to avoid waiting every time the notebook is openend\n",
        "#!zip text_files.zip text_files/*\n",
        "#!unzip text_files.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLtsHbzlX7nF"
      },
      "outputs": [],
      "source": [
        "!sed -n '58,88p;89q' text_files/aliceinwonderland.txt > shingling_test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tXzED-WoliZ"
      },
      "outputs": [],
      "source": [
        "text_files = [\n",
        "    \"text_files/aliceinwonderland.txt\",\n",
        "    \"text_files/dracula.txt\",\n",
        "    \"text_files/drjekyllandmrhyde.txt\",\n",
        "    \"text_files/frankenstein.txt\",\n",
        "    \"text_files/metamorphosis.txt\",\n",
        "    \"text_files/priceandprejudice.txt\",\n",
        "    \"text_files/sherlockholmes.txt\",\n",
        "    \"text_files/thegreatgatsby.txt\",\n",
        "    \"text_files/thepictureofdoriangray.txt\",\n",
        "    \"text_files/theprince.txt\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9ofmWGyOQI"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MTLebJnCV-n"
      },
      "source": [
        "## Shingling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLxlPpSjCX39"
      },
      "outputs": [],
      "source": [
        "class Shingling:\n",
        "\n",
        "  def __init__(self, filename, shingle_length, hash_function):\n",
        "    self.filename = filename\n",
        "    self.shingle_length = shingle_length\n",
        "    self.hash_function = hash_function\n",
        "\n",
        "  # read the file, and clean the text\n",
        "  def _read_file(self):\n",
        "    with open(self.filename, \"r\") as f:\n",
        "      lines = f.readlines()\n",
        "    # we remove all white space and some markdown formatting\n",
        "    whole_text = \"\".join(map(lambda s: s.replace(\" \", \"\").replace(\"_\", \"\"), lines))\n",
        "    return whole_text\n",
        "\n",
        "  # Takes the shingles, hashes them, and places them in a set\n",
        "  def _hash_shingles(self, shingles):\n",
        "    # send each shingle through the provided hash function\n",
        "    return set(map(self.hash_function, shingles))\n",
        "\n",
        "  # store the text in a string and place the k-shingles in a set\n",
        "  def _create_shingles(self):\n",
        "    s = self._read_file()\n",
        "    shingles = set()\n",
        "    # move along the string picking out shingles\n",
        "    for i in range(0, len(s) - self.shingle_length + 1):\n",
        "      shingles.add(s[i:i + self.shingle_length])\n",
        "    return shingles\n",
        "\n",
        "  # create a set of hashed shingles\n",
        "  def create_hashed_shingles(self):\n",
        "    shingles = self._create_shingles()\n",
        "    return self._hash_shingles(shingles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gO99nWTbyD0"
      },
      "outputs": [],
      "source": [
        "# defines a hash function that hashes the string and cuts it down to a 4 byte integer\n",
        "hash_function = lambda string: int(hashlib.sha1(str(string).encode(\"utf-8\")).hexdigest(), 16) % (2 ** 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKdPn6z7pD-S"
      },
      "source": [
        "#### Shingling test\n",
        "Create a shingling class and use it to create a set of hashed shingles from a document. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nno4TbybaGEP"
      },
      "outputs": [],
      "source": [
        "shingling_test = Shingling(\"shingling_test.txt\", 10, hash_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oruD6RkPdGKe"
      },
      "outputs": [],
      "source": [
        "test_shingles = shingling_test.create_hashed_shingles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MJJQndndMJI",
        "outputId": "d63ed5d9-0d66-4473-d1f5-2ee1af026e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1348"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_shingles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHIpBkicobjf"
      },
      "source": [
        "## CompareSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySMxcnwEowBZ"
      },
      "outputs": [],
      "source": [
        "class CompareSets:\n",
        "  # takes two sets and computes their jaccard similarity\n",
        "  def compute_jaccard_similarity(self, set1, set2):\n",
        "    return len(set1.intersection(set2)) / len(set1.union(set2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZBfTp4FpI63"
      },
      "source": [
        "#### CompareSets test\n",
        "Create a CompareSets class and use it to calculate the similarity of two sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCMcBBYCno4W"
      },
      "outputs": [],
      "source": [
        "compare_sets_test = CompareSets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQIUO-nRnrH2",
        "outputId": "e958b806-b41e-4a4a-ee3a-efe88d5f2966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.42857142857142855"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_sets_test.compute_jaccard_similarity({1,2,3,4,5}, {3,4,5,56,6}) # 3 out of 7 = 0.42... are in both sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLPX3mG0qK3S"
      },
      "source": [
        "## MinHashing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez1eR_7dqRCM"
      },
      "outputs": [],
      "source": [
        "class MinHash:\n",
        "\n",
        "  def __init__(self):\n",
        "\t# dict to store the shingles of each document\n",
        "    self.raw_hashes_dict = dict()\n",
        "\n",
        "  # add a document (represented by its shingles set) to the dict\n",
        "  def add_document(self, document_name, shingles):\n",
        "    self.raw_hashes_dict[document_name] = shingles\n",
        "\n",
        "  def remove_document(self, document_name):\n",
        "    del self.raw_hashes_dict[document_name]\n",
        "\n",
        "  def _get_all_shingles(self):\n",
        "    all_shingles = set()\n",
        "    # get all unique shingles from all documents\n",
        "    for shingles in self.raw_hashes_dict.values():\n",
        "      all_shingles = all_shingles.union(shingles)\n",
        "    return all_shingles\n",
        "\n",
        "  # used to compute the minhash signature from the shingles of a document\n",
        "  def _compute_signature(self, permutation_array, shingle_set):\n",
        "    counter = 0\n",
        "    # count how many rows it takes in the permuted matrix until the set contains a shingle\n",
        "    for i in permutation_array:\n",
        "      if i in shingle_set:\n",
        "        return counter\n",
        "      else:\n",
        "        counter += 1\n",
        "\n",
        "  def create_signature_matrix(self, length):\n",
        "    all_shingles = self._get_all_shingles()\n",
        "    # the signature matrix is sparse and thus represented as a list of indices\n",
        "\t# each key in the dict is a document name, value is an empty list\n",
        "    signature_dict = {d:[] for d in self.raw_hashes_dict.keys()}\n",
        "    for i in range(length):\n",
        "      permutation = np.random.permutation(list(all_shingles))\n",
        "      for document_name, shingles in self.raw_hashes_dict.items():\n",
        "        signature_dict[document_name].append(self._compute_signature(permutation, shingles))\n",
        "    return signature_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7LmqRTt_YA"
      },
      "source": [
        "#### MinHashing test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVXPOalquuM3"
      },
      "outputs": [],
      "source": [
        "text_file_shingles_dict = dict()\n",
        "for file_name in text_files:\n",
        "  f_shingling = Shingling(file_name, 10, hash_function)\n",
        "  f_shingles = f_shingling.create_hashed_shingles()\n",
        "  text_file_shingles_dict[file_name] = f_shingles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp-SQAdNumhM"
      },
      "outputs": [],
      "source": [
        "test_shingles_dict = {k:set(list(v)[:random.randrange(10, 20)]) for k,v in text_file_shingles_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8t3_LWYtK9j"
      },
      "outputs": [],
      "source": [
        "min_hash_test = MinHash()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyrif8kfwECB"
      },
      "outputs": [],
      "source": [
        "for k,v in test_shingles_dict.items():\n",
        "  min_hash_test.add_document(k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SIVQ52BwV3J",
        "outputId": "ba485c92-4966-48fa-ce05-add3da19595c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text_files/aliceinwonderland.txt': [9, 10, 0, 15, 6, 4, 4, 6, 8, 4],\n",
              " 'text_files/dracula.txt': [1, 4, 21, 4, 2, 7, 2, 7, 13, 25],\n",
              " 'text_files/drjekyllandmrhyde.txt': [3, 10, 12, 2, 16, 3, 14, 1, 15, 0],\n",
              " 'text_files/frankenstein.txt': [11, 2, 3, 7, 3, 7, 1, 5, 1, 3],\n",
              " 'text_files/metamorphosis.txt': [0, 8, 1, 6, 16, 7, 0, 0, 4, 1],\n",
              " 'text_files/priceandprejudice.txt': [12, 14, 15, 1, 1, 1, 23, 13, 0, 2],\n",
              " 'text_files/sherlockholmes.txt': [5, 3, 22, 5, 0, 0, 9, 4, 0, 5],\n",
              " 'text_files/thegreatgatsby.txt': [6, 0, 4, 0, 8, 5, 20, 2, 10, 15],\n",
              " 'text_files/thepictureofdoriangray.txt': [8, 1, 2, 8, 3, 7, 3, 5, 2, 29],\n",
              " 'text_files/theprince.txt': [4, 4, 10, 9, 14, 2, 6, 3, 7, 19]}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_signature_matrix = min_hash_test.create_signature_matrix(10)\n",
        "test_signature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejLHKj37yV-Q"
      },
      "source": [
        "## CompareSignatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZjolkxByXVS"
      },
      "outputs": [],
      "source": [
        "class CompareSignatures:\n",
        "\n",
        "  def compare_signatures(self, v1, v2):\n",
        "    assert len(v1) == len(v2)\n",
        "    return np.sum(np.array(v1) == np.array(v2)) / len(v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaHPMCwCy0QU"
      },
      "source": [
        "#### CompareSignatures test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQkPatT4y2eL"
      },
      "outputs": [],
      "source": [
        "compare_signatures_test = CompareSignatures()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2YYGY24zDJL",
        "outputId": "80830690-362d-4a72-d13e-33a4f3192100"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compare_signatures_test.compare_signatures([1,2,3,4,5], [1,4,3,7,9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ihQLxd30jbE"
      },
      "source": [
        "## LSH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3jvU2he0kKb"
      },
      "outputs": [],
      "source": [
        "class LSH:\n",
        "\n",
        "  def __init__(self, similarity_threshold, number_of_bands, hash_function):\n",
        "    self.t = similarity_threshold\n",
        "    self.b = number_of_bands\n",
        "    self.h = hash_function\n",
        "\n",
        "  # generate all pairs of names from a list\n",
        "  def _generate_pairs(self, l):\n",
        "    o = []\n",
        "    for i in range(len(l)):\n",
        "      for j in range(i+1, len(l)):\n",
        "        o.append((l[i], l[j]))\n",
        "    return o\n",
        "\n",
        "  def _hash_bands_into_buckets(self, signature_matrix_dict, start_index, end_index):\n",
        "    buckets_dict = dict()\n",
        "\t# iterate over all documents\n",
        "    for k,v in signature_matrix_dict.items():\n",
        "\t  # extract and hash a band from the document's signature\n",
        "      band = v[start_index:end_index]\n",
        "      hash = self.h(list(band))\n",
        "\t  # store the document name in the bucket\n",
        "      if hash in buckets_dict:\n",
        "        buckets_dict[hash].append(k)\n",
        "      else:\n",
        "        buckets_dict[hash] = [k]\n",
        "    return buckets_dict\n",
        "\n",
        "  def generate_candidate_pairs(self, signature_matrix_dict):\n",
        "\t# preparation for the loop\n",
        "    keys = sorted(list(signature_matrix_dict.keys()))\n",
        "    first_key = keys[0]\n",
        "    matrix_length = len(signature_matrix_dict[first_key])\n",
        "    r = matrix_length // self.b\n",
        "    common_buckets_counter_dict = dict()\n",
        "\t# iterate over all bands\n",
        "    for b in range(self.b):\n",
        "      # calculate start and end index for band\n",
        "      i = r * b\n",
        "      j = r * (b + 1)\n",
        "      if matrix_length - j - r <= 0:\n",
        "        j = matrix_length\n",
        "      # get buckets of names for the current band\n",
        "      buckets_dict = self._hash_bands_into_buckets(signature_matrix_dict, i, j)\n",
        "      for bucket in buckets_dict.values():\n",
        "        # buckets with 2 or more documents are candidates, they have at least one band that hashes to the same value\n",
        "        if len(bucket) >= 2:\n",
        "          candidate_pairs = self._generate_pairs(sorted(bucket))\n",
        "          for p in candidate_pairs:\n",
        "            if p in common_buckets_counter_dict:\n",
        "              common_buckets_counter_dict[p] += 1\n",
        "            else:\n",
        "              common_buckets_counter_dict[p] = 1\n",
        "    # normalise counts to fractions\n",
        "    for k,v in common_buckets_counter_dict.items():\n",
        "      common_buckets_counter_dict[k] = v / self.b\n",
        "    # only return the pairs with their similarity fraction above a certain threshold\n",
        "    return {p:v for p,v in common_buckets_counter_dict.items() if v >= self.t}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfvgEReQ0ktm"
      },
      "source": [
        "#### LSH test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH-7UAr30l8r"
      },
      "outputs": [],
      "source": [
        "k_hash_function_test = lambda x: hash_function(x) % 257"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqSBPbF96HUx"
      },
      "outputs": [],
      "source": [
        "lsh_test = LSH(0.1, 5, k_hash_function_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tgZwMtvCU6E",
        "outputId": "f6cb99d7-6580-437b-aec4-fc32202da298"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('text_files/frankenstein.txt', 'text_files/priceandprejudice.txt'): 0.2,\n",
              " ('text_files/thepictureofdoriangray.txt', 'text_files/theprince.txt'): 0.2}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lsh_test.generate_candidate_pairs(test_signature_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAEBU9B7D73c"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "The class below allows to find documents in a corpus of text files stored in a directory.\n",
        "\n",
        "The similarity threshold for the evaluation is set to 0 to firstly show the computed similarity value for all 10 documents in the corpus, and secondly owed to the size difference between test file and corpus files, resulting in very low similarity even for straight out excerpts from corpus files, as used here. With smaller documents the threshold could be and would be necessary to be set much higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxprZ56GCjX8"
      },
      "outputs": [],
      "source": [
        "class FindSimilarDocuments:\n",
        "\n",
        "  def __init__(self, corpus_directory, shingle_length, similarity_threshold, hash_function):\n",
        "    self.corpus_directory = corpus_directory\n",
        "    self.shingle_length = shingle_length\n",
        "    self.similarity_threshold = similarity_threshold\n",
        "    self.hash_function = hash_function\n",
        "\n",
        "    # find all files in the corpus and compute their shingles\n",
        "    file_shingles_dict = dict()\n",
        "    for f in os.listdir(corpus_directory):\n",
        "      file_name = os.path.join(corpus_directory, f)\n",
        "      f_shingling = Shingling(file_name, shingle_length, hash_function)\n",
        "      f_shingles = f_shingling.create_hashed_shingles()\n",
        "      file_shingles_dict[f] = f_shingles\n",
        "    self.file_shingles_dict = file_shingles_dict\n",
        "\n",
        "    # add the corpus files to the MinHashing class for later\n",
        "    min_hash = MinHash()\n",
        "    for k,v in file_shingles_dict.items():\n",
        "      min_hash.add_document(k, v)\n",
        "    self.min_hash = min_hash\n",
        "\n",
        "    self.k_hash_function = lambda x: hash_function(x) % 257\n",
        "\n",
        "  def _create_shingles_from_file(self, path):\n",
        "    shingling = Shingling(path, self.shingle_length, self.hash_function)\n",
        "    shingles = shingling.create_hashed_shingles()\n",
        "    return shingles\n",
        "\n",
        "  def find_similar_documents_jaccard(self, document_path, similarity_t=None):\n",
        "    if similarity_t is None:\n",
        "      similarity_t = self.similarity_threshold\n",
        "    # create shingles for input document\n",
        "    shingles = self._create_shingles_from_file(document_path)\n",
        "    results = []\n",
        "    compare_sets = CompareSets()\n",
        "    # compare with corpus documents using Jaccard similarity of the shingles\n",
        "    for k,v in self.file_shingles_dict.items():\n",
        "      similarity = compare_sets.compute_jaccard_similarity(shingles, v)\n",
        "      if similarity >= similarity_t:\n",
        "        results.append((k, similarity))\n",
        "    return results\n",
        "\n",
        "  def find_similar_documents_lsh(self, document_path, number_of_bands=5, signature_matrix_length=30):\n",
        "    # create shingles for document\n",
        "    shingles = self._create_shingles_from_file(document_path)\n",
        "    # compute signature matrix for corpus and new document\n",
        "    self.min_hash.add_document(document_path, shingles)\n",
        "    start = time.time()\n",
        "    signature_matrix = self.min_hash.create_signature_matrix(signature_matrix_length)\n",
        "    end = time.time()\n",
        "    print(f\"Create signature matrix using MinHashing: {end - start:.5f}s\")\n",
        "    self.min_hash.remove_document(document_path)\n",
        "    # find similar documents using LSH\n",
        "    lsh = LSH(self.similarity_threshold, number_of_bands, self.k_hash_function)\n",
        "    start = time.time()\n",
        "    candidate_pairs = lsh.generate_candidate_pairs(signature_matrix)\n",
        "    end = time.time()\n",
        "    print(f\"Find candidates with LSH: {end - start:.5f}s\")\n",
        "    results = []\n",
        "    # only return the \"similar\" document, not document_path, as we know that one\n",
        "    for (a, b), v in candidate_pairs.items():\n",
        "      if document_path == a or document_path == b:\n",
        "        results.append((a if document_path == b else b, v))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd4R1wnCJGKJ"
      },
      "source": [
        "#### Evaluation program tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoIpwfrRH5sF"
      },
      "outputs": [],
      "source": [
        "fsd = FindSimilarDocuments(corpus_directory=\"text_files\",\n",
        "                           shingle_length=10,\n",
        "                           similarity_threshold=0,\n",
        "                           hash_function=hash_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYYaRDazIRHi",
        "outputId": "81bc09c1-ebfe-4894-c3ae-bc4299ff5205"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('aliceinwonderland.txt', 0.011350145244811182),\n",
              " ('metamorphosis.txt', 0.000523082190478002),\n",
              " ('priceandprejudice.txt', 0.0004067107269954245),\n",
              " ('thepictureofdoriangray.txt', 0.0003285769333018696),\n",
              " ('sherlockholmes.txt', 0.0002934367969743406),\n",
              " ('theprince.txt', 0.00021890049408968665),\n",
              " ('thegreatgatsby.txt', 0.0003799731783638802),\n",
              " ('frankenstein.txt', 0.00026455185516988436),\n",
              " ('drjekyllandmrhyde.txt', 0.0003795464823308999),\n",
              " ('dracula.txt', 0.0002163271751144797)]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fsd.find_similar_documents_jaccard(\"shingling_test.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spi2USRxf2V2"
      },
      "source": [
        "Since the test file was generated by cutting out a part of \"Alice in Wonderland\", we would expect a similarity search to find that as the most similar book by a significant margin. And indeed, it is considered 20-50 times more similar to its source book than to any other, confirming our expectations. Of course, the Jaccard similarity is still very low, as the excerpt is significantly shorter than the original text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6hF4bu4j6uq",
        "outputId": "044b70f6-2de5-4b04-a2a4-6be83a5d7e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create signature matrix using MinHashing: 79.59212s\n",
            "Find candidates with LSH: 0.00027s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('metamorphosis.txt', 1.0)]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fsd.find_similar_documents_lsh(\"text_files/metamorphosis.txt\",\n",
        "                               number_of_bands=5,\n",
        "                               signature_matrix_length=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FZU3IrKj7wT",
        "outputId": "2986a995-71ac-4bec-e098-54e7fabbd05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create signature matrix using MinHashing: 132.35803s\n",
            "Find candidates with LSH: 0.00049s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('aliceinwonderland.txt', 0.2)]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fsd.find_similar_documents_lsh(\"shingling_test.txt\",\n",
        "                               number_of_bands=5,\n",
        "                               signature_matrix_length=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUP7t9AxhaGf"
      },
      "source": [
        "Using LSH for similarity search, we try whether it can find a perfect match, and also the excerpt from a book. It comes as no surprise that inputting a document from its corpus has a 100% match with one of the documents from the corpus. Furthermore, it was able to find the book the test excerpt was taken from.\n",
        "\n",
        "Computing Jaccard similarity and LHS are both very fast, however, the  problem with this implementation is the recomputation of the whole signature matrix for every input, which is necessary here, as the shingles of the input are not known in advance. This is the most time consuming procedure of the LSH process; LSH requires these signatures while Jaccard doesn't. This could be avoided by saving the permutations on the whole shingle-by-document matrix and applying them only to the new input, thus saving a lot of time. When the signature matrix is known, our results indicate that the actual process of finding similar items is faster when using LSH as opposed to computing the Jaccard similarities directly. It should also be noted that the benefits of using LSH are more obvious when the corpus is very large, as the number of candidate pairs will probably be much lower than the number of pairs overall in that case. It is also worth discussing the impact that the number of bands has on the effectiveness of LSH. If the algorithm has a hard time finding candidate pairs, it might be a good idea to increase the number of bands. Indeed, while testing our code, it was sometimes hard to find any candidates at all when this number remained low. This is of course a trade-off between speed and effectiveness as a higher number of bands will negatively impact the runtime of the algorithm. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
